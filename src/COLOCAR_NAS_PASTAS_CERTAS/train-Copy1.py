import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import os
import time
import argparse

from models import BeamNet, BeamNetLarge
from dataloader import PreprocessedBeamDataset
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.amp import autocast, GradScaler

# --- Configura√ß√µes de Caminho Robustas ---
# SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, "..", ".."))

# Caminho para o dataset
N_SAMPLES = 10000

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))

# --- PATHS DE INPUT (Datasets) ---
DATASETS_DIR = os.path.join(PROJECT_ROOT, "dataset")

PREPROC_NPZ = os.path.join(
    DATASETS_DIR, f"beam_dataset_{N_SAMPLES}_samples_preproc.npz"
)

SCALERS_NPZ = os.path.join(DATASETS_DIR, f"beam_scalers_{N_SAMPLES}_samples.npz")

# --- PATHS DE OUTPUT (Resultados) ---
OUTPUTS_DIR = os.path.join(PROJECT_ROOT, "outputs", "treinamentos")

# Nome do experimento/dataset
EXPERIMENT_NAME = f"dataset_viga1D_{N_SAMPLES}_samples"

# Pasta espec√≠fica deste experimento
EXPERIMENT_PATH = os.path.join(OUTPUTS_DIR, EXPERIMENT_NAME)

# Cria o diret√≥rio de output se n√£o existir
os.makedirs(EXPERIMENT_PATH, exist_ok=True)

# --- Configura√ß√£o de Hardware ---
DEVICE = "cuda"


# ============================================================
# NOVA FUN√á√ÉO DE PERDA: SOBOLEV DE 1¬™ ORDEM
# ============================================================
def sobolev_loss(y_pred, y_true, lambda_theta=1.0):
    """
    Calcula o erro do valor (w) e o erro da derivada (theta) separadamente.

    Args:
        y_pred: Tensor [batch, 2] -> (w_pred, theta_pred)
        y_true: Tensor [batch, 2] -> (w_true, theta_true)
        lambda_theta: Peso dado ao erro da derivada (rota√ß√£o).
    """
    # Separa as colunas (0 = Deslocamento, 1 = Rota√ß√£o/Derivada)
    w_pred, theta_pred = y_pred[:, 0], y_pred[:, 1]
    w_true, theta_true = y_true[:, 0], y_true[:, 1]

    # Calcula MSE individualmente
    loss_w = torch.mean((w_pred - w_true) ** 2)
    loss_theta = torch.mean((theta_pred - theta_true) ** 2)

    # Retorna a soma ponderada
    return loss_w + (lambda_theta * loss_theta)


def main(args):
    # Pega LR e batch_size da linha de comando
    LR = args.LR_inicial
    BATCH_SIZE = args.batch_size
    MIN_LR = LR / 50
    LAMBDA_THETA = args.lambda_theta
    HIDDEN_DIM = args.hidden_dim
    DROPOUT_P = args.dropout
    WEIGHT_DECAY = args.weight_decay
    EPOCHS = args.epochs

    print(f" Iniciando treinamento em: {DEVICE}")
    print(f" Usando Sobolev Loss com Lambda_Theta = {LAMBDA_THETA}")
    print(
        f" Configura√ß√£o: N_SAMPLES={N_SAMPLES} | batch_size={BATCH_SIZE} | LR_inicial={LR:.2e}"
    )

    # --------------------------------------------------
    # 1. Carregar Dados (medir tempo)
    # --------------------------------------------------
    full_dataset = PreprocessedBeamDataset(
        PREPROC_NPZ,
        scalers_path=SCALERS_NPZ,  # opcional, mas √∫til se voc√™ quiser usar depois
    )

    # Split 80/20
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_data, val_data = torch.utils.data.random_split(
        full_dataset, [train_size, val_size]
    )

    # --------------------------------------------------
    # 2. DataLoaders (medir tempo)
    # --------------------------------------------------
    train_loader = DataLoader(
        train_data,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=0,
        pin_memory=True,
        persistent_workers=False,
        drop_last=True,
    )

    val_loader = DataLoader(
        val_data,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=0,
        pin_memory=True,
        persistent_workers=False,
    )

    print(f" Dados: {len(full_dataset)} pontos totais")
    print(f" Treino: {train_size} | Valida√ß√£o Interna: {val_size}")

    # 4. Modelo e Otimizador
    model = BeamNet(
        input_dim=5, output_dim=2, hidden_dim=HIDDEN_DIM, dropout_p=DROPOUT_P
    ).to(DEVICE)
    # model = BeamNetLarge(...)

    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

    scaler = GradScaler("cuda")

    # Scheduler principal: Cosine Annealing
    scheduler = CosineAnnealingLR(
        optimizer,
        T_max=EPOCHS,  # per√≠odo completo do cosseno
        eta_min=MIN_LR,  # LR m√≠nimo
    )

    def sched_step():
        scheduler.step()

    loss_history = {"train": [], "val": []}

    # m√©tricas globais que queremos no final
    best_val_sobolev_loss = float("inf")
    best_val_w_mse = float("inf")
    epoch_of_best_val = -1

    final_val_sobolev_loss = None
    final_val_w_mse = None

    # 6. Loop de Treino
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0.0

        for X_batch, y_batch in train_loader:
            # -----------------------------
            # Data + to(device)
            # -----------------------------
            X_batch = X_batch.cuda(non_blocking=True)
            y_batch = y_batch.cuda(non_blocking=True)

            optimizer.zero_grad(set_to_none=True)

            # -----------------------------
            # Forward
            # -----------------------------
            with autocast("cuda", dtype=torch.bfloat16):
                y_pred = model(X_batch)
                loss = sobolev_loss(y_pred, y_batch, lambda_theta=LAMBDA_THETA)

            # -----------------------------
            # Backward
            # -----------------------------
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            # -----------------------------
            # Step do otimizador
            # -----------------------------
            scaler.step(optimizer)
            scaler.update()

            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)

        # --------------------------------------------------
        # Valida√ß√£o + MSE s√≥ de w
        # --------------------------------------------------
        model.eval()
        val_loss = 0.0
        val_w_mse_sum = 0.0

        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch = X_batch.cuda(non_blocking=True)
                y_batch = y_batch.cuda(non_blocking=True)

                with autocast("cuda", dtype=torch.bfloat16):
                    y_pred = model(X_batch)
                    loss = sobolev_loss(y_pred, y_batch, lambda_theta=LAMBDA_THETA)

                    # MSE apenas de w (coluna 0)
                    w_pred = y_pred[:, 0].float()
                    w_true = y_batch[:, 0].float()
                    mse_w_batch = torch.mean((w_pred - w_true) ** 2)

                val_loss += loss.item()
                val_w_mse_sum += mse_w_batch.item()

        avg_val_loss = val_loss / len(val_loader)
        avg_val_w_mse = val_w_mse_sum / len(val_loader)

        # Atualiza scheduler
        sched_step()
        current_lr = optimizer.param_groups[0]["lr"]

        loss_history["train"].append(avg_train_loss)
        loss_history["val"].append(avg_val_loss)

        # Atualiza m√©tricas globais (melhor √©poca)
        if avg_val_loss < best_val_sobolev_loss:
            best_val_sobolev_loss = avg_val_loss
            best_val_w_mse = avg_val_w_mse
            epoch_of_best_val = epoch + 1  # 1-based

        # Atualiza m√©tricas finais (√∫ltima √©poca)
        final_val_sobolev_loss = avg_val_loss
        final_val_w_mse = avg_val_w_mse

    # --------------------------------------------------
    # 7. M√©tricas agregadas no final
    # --------------------------------------------------

    print("\n================= RESUMO DO EXPERIMENTO =================")
    print(f"batch_size = {BATCH_SIZE}")
    print(f"LR_inicial = {LR:.4e}")
    print(f"best_val_sobolev_loss = {best_val_sobolev_loss:.6e}")
    print(f"best_val_w_mse        = {best_val_w_mse:.6e}")
    print(f"epoch_of_best_val     = {epoch_of_best_val}")
    print(f"final_val_sobolev_loss = {final_val_sobolev_loss:.6e}")
    print(f"final_val_w_mse        = {final_val_w_mse:.6e}")
    print("=========================================================\n")

    # >>> √öNICA MUDAN√áA PEDIDA: LINHA NO FORMATO CSV <<<
    print(
        "best_val_sobolev_loss;best_val_w_mse;epoch_of_best_val;final_val_sobolev_loss;final_val_w_mse"
    )
    print(
        f"{best_val_sobolev_loss};{best_val_w_mse};{epoch_of_best_val};{final_val_sobolev_loss};{final_val_w_mse}"
    )

    # 8. Salvar Modelo
    torch.save(model.state_dict(), os.path.join(EXPERIMENT_PATH, "beamnet_model.pth"))
    print("üíæ Modelo salvo!")

    # 9. Plotar Loss
    plt.figure(figsize=(8, 5))
    plt.plot(loss_history["train"], label="Treino")
    plt.plot(loss_history["val"], label="Valida√ß√£o")
    plt.yscale("log")
    plt.title(f"Curva de Converg√™ncia (Sobolev Lambda = {LAMBDA_THETA})")
    plt.xlabel("√âpocas")
    plt.ylabel("Loss Ponderada")
    plt.legend()
    plt.grid(True, which="both", ls="--")
    plt.savefig(os.path.join(EXPERIMENT_PATH, "training_loss.png"))
    print("üìä Gr√°fico de Loss salvo!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--LR_inicial",
        type=float,
        default=0.00102447017812162,
        help="Learning rate inicial (float), ex: 3e-4",
    )
    parser.add_argument(
        "--batch_size", type=int, default=576, help="Tamanho do batch (int), ex: 256"
    )
    parser.add_argument("--lambda_theta", type=float, default=1)
    parser.add_argument("--hidden_dim", type=int, default=448)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--weight_decay", type=float, default=0.0000118777715916058)
    parser.add_argument("--epochs", type=int, default=300)

    args = parser.parse_args()
    main(args)
